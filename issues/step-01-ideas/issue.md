# Step 1: Ideas → Topics → Titles (Per Segment)

**Status:** Not Started  
**Priority:** High  
**Dependencies:** Step 0 (Research Prototypes)

## Overview

Generate raw ideas from multiple sources (Reddit stories, trending topics, alternative platforms), cluster them into topics, and convert topics into clickable titles for each target segment (gender/age combination).

## Target Audience
- Segments: `women/{age}` and `men/{age}`
- Age buckets: `10-13`, `14-17`, `18-23`

## Checklist

### 1.0 Source Collection (Reddit & Alternative Sources)

- [ ] **Reddit Story Mining**
  - [ ] Set up Reddit API authentication (PRAW library)
  - [ ] Target subreddits by segment/age:
    - For `women/10-13`: r/TrueOffMyChest, r/relationships, r/AmItheAsshole (filtered)
    - For `women/14-17`: r/teenagers, r/AmItheAsshole, r/TrueOffMyChest
    - For `women/18-23`: r/relationships, r/dating_advice, r/confession
    - For `men/10-13`: r/teenagers, r/stories (filtered)
    - For `men/14-17`: r/teenagers, r/confession, r/TrueOffMyChest
    - For `men/18-23`: r/relationships, r/AskMen, r/confession
  - [ ] Scrape top posts (last 7 days, 50-100 posts per subreddit)
  - [ ] Filter by:
    - Minimum upvotes (e.g., 500+)
    - Minimum engagement (comments, awards)
    - Age-appropriateness using content filters
    - Story structure (has beginning, conflict, resolution)
  - [ ] Extract story summaries and key elements
  - [ ] Save: `/sources/reddit/{segment}/{age}/YYYYMMDD_reddit_stories.json`

- [ ] **Alternative Story Sources**
  - [ ] **Quora**: Trending questions and answers
    - Use Quora web scraping or unofficial API
    - Target topics: relationships, life advice, personal experiences
    - Save: `/sources/quora/{segment}/{age}/YYYYMMDD_quora.json`
  - [ ] **Twitter/X**: Viral story threads
    - Search for story threads with high engagement
    - Filter by hashtags: #storytime, #thread, #AITA
    - Save: `/sources/twitter/{segment}/{age}/YYYYMMDD_twitter.json`
  - [ ] **Medium**: Popular personal stories
    - Scrape trending stories from relevant tags
    - Focus on: relationships, life lessons, personal growth
    - Save: `/sources/medium/{segment}/{age}/YYYYMMDD_medium.json`
  - [ ] **Tumblr**: Story posts and experiences
    - Search for popular story posts
    - Filter by notes/reblogs
    - Save: `/sources/tumblr/{segment}/{age}/YYYYMMDD_tumblr.json`
  - [ ] **YouTube Community Posts**: Story snippets
    - Extract text from high-engagement community posts
    - Focus on storytelling channels
    - Save: `/sources/youtube/{segment}/{age}/YYYYMMDD_youtube.json`

- [ ] **Story Quality Assessment**
  - [ ] Score each story (0-100) based on:
    - Emotional impact (drama, conflict, resolution)
    - Relatability to target audience
    - Viral potential (existing engagement metrics)
    - Story completeness (clear narrative arc)
    - Age-appropriateness
  - [ ] Deduplicate similar stories across sources
  - [ ] Rank stories by quality score
  - [ ] Save top 50 stories per segment: `/sources/ranked/{segment}/{age}/YYYYMMDD_top_stories.json`

### 1.1 Ideas Generation (Enhanced)
- [ ] Generate **≥20 raw ideas** per segment from multiple sources:
  - [ ] 10-15 ideas adapted from Reddit/alternative sources
  - [ ] 5-10 original ideas generated by LLM (Qwen2.5 or Llama3.1)
- [ ] Save to: `/ideas/{segment}/{age}/YYYYMMDD_ideas.md`
- [ ] Include source attribution for adapted ideas
- [ ] Ideas should be age-appropriate and gender-relevant
- [ ] Each idea should include:
  - Brief description (2-3 sentences)
  - Source (Reddit/Quora/Twitter/LLM/etc.)
  - Original post link (if applicable)
  - Estimated viral potential score

### 1.2 Topics (Clustering)
- [ ] Cluster ideas into **≥8 topics** per segment
- [ ] Save JSON to: `/topics/{segment}/{age}/YYYYMMDD_topics.json`
- [ ] Each topic should group 2-4 related ideas
- [ ] Include topic metadata (theme, keywords, potential)

### 1.3 Titles Generation
- [ ] Convert topics → **≥10 clickable titles** per segment
- [ ] Save JSON to: `/titles/{segment}/{age}/YYYYMMDD_titles.json`
- [ ] Titles should be engaging and platform-optimized
- [ ] Include title metadata (topic_id, created timestamp)

## JSON Schema Examples

### Reddit Stories JSON (`YYYYMMDD_reddit_stories.json`)
```json
{
  "segment": "women|men",
  "age_bucket": "10-13|14-17|18-23",
  "collected_at": "2024-01-01T12:00:00Z",
  "stories": [
    {
      "id": "reddit-001",
      "title": "I discovered my best friend's secret",
      "subreddit": "r/TrueOffMyChest",
      "post_id": "abc123",
      "url": "https://reddit.com/r/TrueOffMyChest/comments/abc123",
      "author": "throwaway123",
      "posted_date": "2024-01-01T10:00:00Z",
      "content_summary": "Brief summary of the story...",
      "engagement": {
        "upvotes": 5420,
        "comments": 342,
        "awards": 12
      },
      "quality_score": 87,
      "tags": ["friendship", "secrets", "betrayal"],
      "age_appropriate": true,
      "word_count": 856
    }
  ]
}
```

### Ranked Stories JSON (`YYYYMMDD_top_stories.json`)
```json
{
  "segment": "women|men",
  "age_bucket": "10-13|14-17|18-23",
  "collected_at": "2024-01-01T12:00:00Z",
  "total_sources_checked": 5,
  "total_stories_found": 247,
  "top_stories": [
    {
      "rank": 1,
      "story_id": "reddit-042",
      "source": "reddit",
      "title": "The letter that changed everything",
      "quality_score": 94,
      "viral_potential": 92,
      "emotional_impact": 95,
      "relatability": 91,
      "narrative_quality": 96,
      "url": "https://reddit.com/...",
      "tags": ["mystery", "family", "discovery"]
    }
  ]
}
```

### Topics JSON (`YYYYMMDD_topics.json`)
```json
{
  "segment": "women|men",
  "age_bucket": "10-13|14-17|18-23",
  "topics": [
    {
      "id": "topic-001",
      "name": "Mystery and Secrets",
      "idea_ids": ["idea-001", "idea-005", "idea-012"],
      "keywords": ["mystery", "secrets", "hidden"],
      "created_utc": "2024-01-01T12:00:00Z"
    }
  ]
}
```

### Titles JSON (`YYYYMMDD_titles.json`)
```json
{
  "segment": "women|men",
  "age_bucket": "10-13|14-17|18-23",
  "titles": [
    {
      "id": "uuid",
      "title": "The Secret That Changed Everything",
      "topic_ids": ["topic-001"],
      "created_utc": "2024-01-01T12:00:00Z"
    }
  ]
}
```

## Acceptance Criteria

- [ ] Reddit API integration working (authenticated, rate-limited)
- [ ] Alternative source scrapers implemented (at least 2 platforms)
- [ ] Story collection automated for all 6 segments
- [ ] Quality scoring algorithm implemented
- [ ] Source files exist: `/sources/{platform}/{segment}/{age}/`
- [ ] Ranked stories exist: `/sources/ranked/{segment}/{age}/`
- [ ] Ideas files exist for all segment/age combinations
- [ ] At least 20 ideas per segment (10-15 adapted + 5-10 original)
- [ ] Topics JSON files exist with ≥8 topics each
- [ ] Titles JSON files exist with ≥10 titles each
- [ ] All files follow naming convention `YYYYMMDD_*.{md,json}`
- [ ] Content is age-appropriate and segment-relevant
- [ ] Source attribution included for adapted content

## Implementation Notes

### Reddit API Setup (PRAW)

```python
import praw
from datetime import datetime, timedelta

# Initialize Reddit API
reddit = praw.Reddit(
    client_id='your_client_id',
    client_secret='your_client_secret',
    user_agent='StoryGenerator/1.0'
)

def scrape_subreddit_stories(subreddit_name, segment, age, min_score=500):
    """Scrape stories from a subreddit."""
    subreddit = reddit.subreddit(subreddit_name)
    stories = []
    
    # Get top posts from last week
    for submission in subreddit.top(time_filter='week', limit=100):
        if submission.score >= min_score:
            story = {
                'id': f'reddit-{submission.id}',
                'title': submission.title,
                'subreddit': f'r/{subreddit_name}',
                'post_id': submission.id,
                'url': submission.url,
                'author': str(submission.author),
                'posted_date': datetime.fromtimestamp(submission.created_utc).isoformat(),
                'content_summary': submission.selftext[:500],
                'engagement': {
                    'upvotes': submission.score,
                    'comments': submission.num_comments,
                    'awards': submission.total_awards_received
                }
            }
            stories.append(story)
    
    return stories
```

### Alternative Source Scraping

```python
# Quora scraping (using beautifulsoup4 + requests)
import requests
from bs4 import BeautifulSoup

def scrape_quora_questions(topic, limit=50):
    """Scrape trending Quora questions."""
    # Implementation with rate limiting and error handling
    pass

# Twitter/X API (using tweepy)
import tweepy

def search_twitter_story_threads(keywords, segment, age):
    """Search for story threads on Twitter."""
    # Implementation with Twitter API v2
    pass
```

### Quality Scoring Algorithm

```python
def score_story_quality(story, segment, age):
    """Score a story for viral potential and quality."""
    scores = {}
    
    # Emotional impact (based on keywords, engagement)
    scores['emotional_impact'] = calculate_emotional_score(story)
    
    # Relatability (based on topic relevance to segment/age)
    scores['relatability'] = calculate_relatability(story, segment, age)
    
    # Viral potential (based on existing engagement metrics)
    scores['viral_potential'] = calculate_viral_score(story['engagement'])
    
    # Narrative quality (story structure, completeness)
    scores['narrative_quality'] = analyze_narrative_structure(story['content_summary'])
    
    # Overall score (weighted average)
    overall = (
        scores['emotional_impact'] * 0.3 +
        scores['relatability'] * 0.25 +
        scores['viral_potential'] * 0.25 +
        scores['narrative_quality'] * 0.2
    )
    
    return overall, scores
```

### Content Filtering

```python
def is_age_appropriate(content, age_bucket):
    """Filter content for age appropriateness."""
    # Check for inappropriate keywords
    inappropriate_keywords = load_age_filter_keywords(age_bucket)
    
    # Sentiment analysis
    sentiment = analyze_sentiment(content)
    
    # Content safety check
    safety_score = check_content_safety(content)
    
    return safety_score > 0.8 and not contains_keywords(content, inappropriate_keywords)
```

## Related Files

### Source Directories (New)
- `/sources/reddit/{segment}/{age}/` - Reddit scraped stories
- `/sources/quora/{segment}/{age}/` - Quora content
- `/sources/twitter/{segment}/{age}/` - Twitter story threads
- `/sources/medium/{segment}/{age}/` - Medium articles
- `/sources/tumblr/{segment}/{age}/` - Tumblr posts
- `/sources/youtube/{segment}/{age}/` - YouTube community posts
- `/sources/ranked/{segment}/{age}/` - Ranked top stories

### Output Directories
- `/ideas/{segment}/{age}/` - Ideas directory
- `/topics/{segment}/{age}/` - Topics directory  
- `/titles/{segment}/{age}/` - Titles directory

### Configuration
- `/config/pipeline.yaml` - LLM configuration
- `/config/sources.yaml` - API credentials and source configuration

## Required Dependencies

### Python Packages
```bash
pip install praw  # Reddit API
pip install tweepy  # Twitter API
pip install requests beautifulsoup4  # Web scraping
pip install nltk textblob  # Text analysis and sentiment
pip install scikit-learn  # Clustering and quality scoring
```

### API Keys Required
- Reddit API (client_id, client_secret)
- Twitter/X API (bearer token)
- Optional: Quora, Medium (may require scraping)

## Configuration Example

`/config/sources.yaml`:
```yaml
reddit:
  client_id: "your_reddit_client_id"
  client_secret: "your_reddit_client_secret"
  user_agent: "StoryGenerator/1.0"
  rate_limit_delay: 2  # seconds between requests
  subreddits:
    women:
      "10-13": ["TrueOffMyChest", "relationships", "AmItheAsshole"]
      "14-17": ["teenagers", "AmItheAsshole", "TrueOffMyChest"]
      "18-23": ["relationships", "dating_advice", "confession"]
    men:
      "10-13": ["teenagers", "stories"]
      "14-17": ["teenagers", "confession", "TrueOffMyChest"]
      "18-23": ["relationships", "AskMen", "confession"]

twitter:
  bearer_token: "your_twitter_bearer_token"
  search_keywords:
    - "#storytime"
    - "#AITA"
    - "#thread"
  max_results: 100

quality_thresholds:
  min_upvotes: 500
  min_comments: 50
  min_quality_score: 70
  min_viral_potential: 65
```

## Microstep Validation

Use the MicrostepValidator for tracking:
- Step 2: ideas (enhanced with source collection)
- Step 3: topics
- Step 4: titles

```python
from Tools.MicrostepValidator import update_microstep_progress

# After source collection
update_microstep_progress(
    2, 'completed', 
    'Collected stories from Reddit and alternative sources',
    gender='women', age='18-23',
    artifacts=['reddit_stories.json', 'ranked_stories.json']
)

# After ideas generation
update_microstep_progress(
    2, 'completed',
    'Generated 20 ideas (15 adapted + 5 original)',
    gender='women', age='18-23'
)
```

Comment `@copilot check` when all artifacts are created.

## Ethical Considerations

### Content Attribution
- Always maintain source attribution for adapted stories
- Do not claim original authorship of Reddit/social media content
- Transform stories significantly to create derivative works
- Respect original authors' intent and privacy

### Privacy & Anonymization
- Remove identifying information from adapted stories
- Change names, locations, and specific details
- Do not use content from private/restricted communities
- Respect deleted posts (do not use cached versions)

### Age Appropriateness
- Implement strict content filters for younger age buckets
- Review automated filtering with manual spot-checks
- Err on the side of caution for borderline content
- Maintain separate filter lists per age group

### Platform Terms of Service
- Comply with Reddit API Terms of Service
- Respect rate limits and API quotas
- Follow robots.txt for web scraping
- Use official APIs when available

### Fair Use & Copyright
- Adapt stories into transformative works
- Add creative elements and narrative structure
- Do not reproduce content verbatim
- Consult legal counsel for specific use cases

## Notes

- Use consistent segment folders: `women/10-13`, `women/14-17`, `women/18-23`, `men/10-13`, `men/14-17`, `men/18-23`
- Total combinations: 6 (2 genders × 3 age buckets)
- Source collection can be automated daily/weekly via cron jobs
- Implement caching to avoid re-scraping the same content
- Monitor API quotas and adjust collection frequency accordingly
- Consider using proxies for web scraping to avoid IP blocking
